#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ç‰ˆæœ¬çš„æ•°æ®æ”¶é›†è„šæœ¬
ä¸ä¾èµ–ç½‘ç»œè¯·æ±‚ï¼Œç”¨äºæµ‹è¯•è„šæœ¬é€»è¾‘
"""

import json
import re
from datetime import datetime, timedelta
import os

# é…ç½®ä¿¡æ¯
COMPANIES = ['å¥½æœªæ¥', 'å¸Œæœ›å­¦', 'å­¦è€Œæ€', 'æ–°ä¸œæ–¹', 'ä½œä¸šå¸®', 'é«˜é€”']
CATEGORIES = {
    'æ•™ç ”': ['æ•™ç ”', 'æ•™å­¦', 'è¯¾ç¨‹', 'æ•™å¸ˆ', 'æ•™å­¦æ–¹æ³•', 'è¯¾ç¨‹ä½“ç³»'],
    'AIæ•™è‚²': ['AI', 'äººå·¥æ™ºèƒ½', 'æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'ç®—æ³•', 'ä¸ªæ€§åŒ–'],
    'åŠ¨ç”»è§†é¢‘': ['åŠ¨ç”»', 'è§†é¢‘', '3D', 'åˆ¶ä½œ', 'å†…å®¹åº“'],
    'å¸‚åœºè¥é”€': ['è¥é”€', 'æ¨å¹¿', 'æ´»åŠ¨', 'æ‹›ç”Ÿ', 'å¸‚åœº'],
    'å“ç‰Œå…¬å…³': ['å“ç‰Œ', 'å…¬å…³', 'ä¼ æ’­', 'å½¢è±¡', 'è®¤å¯'],
    'ç”¨æˆ·è¿è¥': ['ç”¨æˆ·', 'è¿è¥', 'æ´»è·ƒ', 'ç•™å­˜', 'è½¬åŒ–']
}

class NewsCollector:
    def __init__(self):
        self.collected_news = []
        
    def get_current_week(self):
        """è·å–å½“å‰å‘¨ä¿¡æ¯"""
        today = datetime.now()
        # è®¡ç®—æ˜¯ç¬¬å‡ å‘¨ï¼ˆISOå‘¨ï¼‰
        year, week, weekday = today.isocalendar()
        week_start = today - timedelta(days=weekday - 1)
        week_end = week_start + timedelta(days=6)
        
        week_str = f"{year}å¹´ç¬¬{week}å‘¨ ({week_start.strftime('%mæœˆ%dæ—¥')}-{week_end.strftime('%mæœˆ%dæ—¥')})"
        return week_str, week_start.strftime('%Y-%m-%d'), week_end.strftime('%Y-%m-%d')
    
    def search_keywords(self, text, keywords):
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«å…³é”®è¯"""
        text_lower = text.lower()
        for keyword in keywords:
            if keyword.lower() in text_lower:
                return True
        return False
    
    def categorize_news(self, title, content):
        """æ ¹æ®æ ‡é¢˜å’Œå†…å®¹åˆ†ç±»"""
        categories = []
        full_text = title + ' ' + content
        
        for category, keywords in CATEGORIES.items():
            if self.search_keywords(full_text, keywords):
                categories.append(category)
        
        return categories if categories else ['å…¶ä»–']
    
    def determine_priority(self, title, content, categories):
        """åˆ¤æ–­ä¼˜å…ˆçº§ï¼ˆé‡ç‚¹/æ¬¡é‡ç‚¹ï¼‰"""
        # é‡ç‚¹å…³é”®è¯
        high_priority_keywords = [
            'çªç ´', 'åˆ›æ–°', 'å‘å¸ƒ', 'ä¸Šçº¿', 'è·å¾—è®¤å¯', 'é‡Œç¨‹ç¢‘', 
            'çªç ´', 'åƒä¸‡', 'ç™¾ä¸‡', 'å›½é™…', 'å¹´åº¦', 'é‡å¤§'
        ]
        
        # æ¬¡é‡ç‚¹å…³é”®è¯
        normal_priority_keywords = [
            'æ´»åŠ¨', 'ç­–ç•¥', 'è°ƒæ•´', 'ä¼˜åŒ–', 'æ›´æ–°', 'å¯åŠ¨'
        ]
        
        full_text = (title + ' ' + content).lower()
        
        # æ£€æŸ¥é‡ç‚¹å…³é”®è¯
        for keyword in high_priority_keywords:
            if keyword in full_text:
                return 'é‡ç‚¹'
        
        # æ£€æŸ¥æ¬¡é‡ç‚¹å…³é”®è¯
        for keyword in normal_priority_keywords:
            if keyword in full_text:
                return 'æ¬¡é‡ç‚¹'
        
        # é»˜è®¤æ ¹æ®åˆ†ç±»åˆ¤æ–­
        if 'AIæ•™è‚²' in categories or 'æ•™ç ”' in categories:
            return 'é‡ç‚¹'
        
        return 'æ¬¡é‡ç‚¹'
    
    def extract_company(self, title, content):
        """ä»æ–‡æœ¬ä¸­æå–å…¬å¸åç§°"""
        full_text = title + ' ' + content
        for company in COMPANIES:
            if company in full_text:
                return company
        return None
    
    def create_test_news(self):
        """åˆ›å»ºæµ‹è¯•æ–°é—»æ•°æ®"""
        test_news = [
            {
                'company': 'å¥½æœªæ¥',
                'title': 'å¥½æœªæ¥AIæ•™è‚²äº§å“è·å¾—æ–°çªç ´ï¼Œç”¨æˆ·æ•°æŒç»­å¢é•¿',
                'content': 'å¥½æœªæ¥æœ¬å‘¨åœ¨AIæ•™è‚²äº§å“æ–¹é¢è·å¾—æ–°çªç ´ï¼Œäº§å“ç”¨æˆ·æ•°æŒç»­å¢é•¿ï¼Œé€šè¿‡æŒç»­çš„æŠ€æœ¯åˆ›æ–°å’Œå†…å®¹ä¼˜åŒ–ï¼Œè·å¾—äº†ç”¨æˆ·çš„å¹¿æ³›è®¤å¯ã€‚',
                'priority': 'é‡ç‚¹',
                'categories': ['AIæ•™è‚²', 'ç”¨æˆ·è¿è¥'],
                'date': datetime.now().strftime('%Y-%m-%d'),
                'source': 'https://example.com/test1'
            },
            {
                'company': 'å­¦è€Œæ€',
                'title': 'å­¦è€Œæ€æ•™ç ”å›¢é˜Ÿå‘å¸ƒæ–°è¯¾ç¨‹ä½“ç³»',
                'content': 'å­¦è€Œæ€æ•™ç ”å›¢é˜Ÿæœ¬å‘¨å‘å¸ƒäº†å…¨æ–°çš„è¯¾ç¨‹ä½“ç³»ï¼Œé’ˆå¯¹ä¸åŒå¹´é¾„æ®µå’Œå­¦ä¹ æ°´å¹³çš„å­¦ç”Ÿè®¾è®¡äº†æ›´åŠ ç§‘å­¦çš„æ•™å­¦æ–¹æ¡ˆã€‚',
                'priority': 'é‡ç‚¹',
                'categories': ['æ•™ç ”'],
                'date': datetime.now().strftime('%Y-%m-%d'),
                'source': 'https://example.com/test2'
            },
            {
                'company': 'æ–°ä¸œæ–¹',
                'title': 'æ–°ä¸œæ–¹å¯åŠ¨ç§‹å­£è¥é”€æ´»åŠ¨',
                'content': 'æ–°ä¸œæ–¹æœ¬å‘¨å¯åŠ¨äº†ç§‹å­£æ‹›ç”Ÿè¥é”€æ´»åŠ¨ï¼Œæ¨å‡ºå¤šé¡¹ä¼˜æƒ æ”¿ç­–å’Œè¯•å¬è¯¾ç¨‹ï¼Œæ—¨åœ¨å¸å¼•æ›´å¤šå­¦ç”ŸæŠ¥åã€‚',
                'priority': 'æ¬¡é‡ç‚¹',
                'categories': ['å¸‚åœºè¥é”€', 'ç”¨æˆ·è¿è¥'],
                'date': datetime.now().strftime('%Y-%m-%d'),
                'source': 'https://example.com/test3'
            },
            {
                'company': 'ä½œä¸šå¸®',
                'title': 'ä½œä¸šå¸®åŠ¨ç”»è§†é¢‘å†…å®¹åº“æ›´æ–°',
                'content': 'ä½œä¸šå¸®æœ¬å‘¨æ›´æ–°äº†åŠ¨ç”»è§†é¢‘å†…å®¹åº“ï¼Œæ–°å¢äº†200å¤šä¸ªæ•™å­¦è§†é¢‘ï¼Œæ¶µç›–å°å­¦åˆ°é«˜ä¸­çš„å„ä¸ªå­¦ç§‘ã€‚',
                'priority': 'æ¬¡é‡ç‚¹',
                'categories': ['åŠ¨ç”»è§†é¢‘'],
                'date': datetime.now().strftime('%Y-%m-%d'),
                'source': 'https://example.com/test4'
            },
            {
                'company': 'é«˜é€”',
                'title': 'é«˜é€”å“ç‰Œå…¬å…³æ´»åŠ¨è·å¾—è‰¯å¥½åå“',
                'content': 'é«˜é€”æœ¬å‘¨çš„å“ç‰Œå…¬å…³æ´»åŠ¨åœ¨ç¤¾äº¤åª’ä½“ä¸Šè·å¾—è‰¯å¥½åå“ï¼Œæå‡äº†å“ç‰ŒçŸ¥ååº¦å’Œç”¨æˆ·å¥½æ„Ÿåº¦ã€‚',
                'priority': 'æ¬¡é‡ç‚¹',
                'categories': ['å“ç‰Œå…¬å…³'],
                'date': datetime.now().strftime('%Y-%m-%d'),
                'source': 'https://example.com/test5'
            },
            {
                'company': 'å¸Œæœ›å­¦',
                'title': 'å¸Œæœ›å­¦ç”¨æˆ·è¿è¥æ•°æ®åˆ›æ–°é«˜',
                'content': 'å¸Œæœ›å­¦æœ¬å‘¨çš„ç”¨æˆ·è¿è¥æ•°æ®åˆ›æ–°é«˜ï¼Œæ—¥æ´»è·ƒç”¨æˆ·æ•°æŒç»­å¢é•¿ï¼Œç”¨æˆ·å¹³å‡ä½¿ç”¨æ—¶é•¿è¾ƒä¸Šæœˆå¢é•¿ã€‚',
                'priority': 'æ¬¡é‡ç‚¹',
                'categories': ['ç”¨æˆ·è¿è¥'],
                'date': datetime.now().strftime('%Y-%m-%d'),
                'source': 'https://example.com/test6'
            }
        ]
        return test_news
    
    def update_data_file(self, news_list):
        """æ›´æ–°data.jsæ–‡ä»¶"""
        week_str, week_start, week_end = self.get_current_week()
        
        # è¯»å–ç°æœ‰æ•°æ®
        data_file = os.path.join(os.path.dirname(__file__), 'data.js')
        
        existing_data = []
        if os.path.exists(data_file):
            try:
                with open(data_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # æå–JSONæ•°æ®
                    json_match = re.search(r'const newsData = (\[.*?\]);', content, re.DOTALL)
                    if json_match:
                        existing_data = json.loads(json_match.group(1))
            except Exception as e:
                print(f"è¯»å–ç°æœ‰æ•°æ®æ—¶å‡ºé”™: {e}")
        
        # æ£€æŸ¥æœ¬å‘¨æ•°æ®æ˜¯å¦å·²å­˜åœ¨
        current_week_data = None
        for week_data in existing_data:
            if week_data['week'] == week_str:
                current_week_data = week_data
                break
        
        # å¦‚æœå­˜åœ¨ï¼Œåˆå¹¶æ–°é—»ï¼›å¦‚æœä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„
        if current_week_data:
            # åˆå¹¶æ–°é—»ï¼Œå»é‡
            existing_titles = {n['title'][:50] for n in current_week_data['news']}
            new_news = [n for n in news_list if n['title'][:50] not in existing_titles]
            current_week_data['news'].extend(new_news)
            print(f"æœ¬å‘¨å·²æœ‰æ•°æ®ï¼Œæ–°å¢ {len(new_news)} æ¡æ–°é—»")
        else:
            # åˆ›å»ºæ–°çš„å‘¨æ•°æ®
            new_week_data = {
                'week': week_str,
                'news': news_list
            }
            existing_data.append(new_week_data)
            print(f"åˆ›å»ºæ–°çš„å‘¨æ•°æ®: {week_str}")
        
        # æŒ‰å‘¨æ’åºï¼ˆæœ€æ–°çš„åœ¨åé¢ï¼‰
        existing_data.sort(key=lambda x: x['week'])
        
        # åªä¿ç•™æœ€è¿‘13å‘¨çš„æ•°æ®
        existing_data = existing_data[-13:]
        
        # å†™å…¥æ–‡ä»¶
        try:
            with open(data_file, 'w', encoding='utf-8') as f:
                f.write('// data.js - æ–°é—»æ•°æ®\n')
                f.write('// 2025å¹´ç¬¬å››å­£åº¦ï¼ˆ10æœˆ-12æœˆï¼‰æ•™åŸ¹è¡Œä¸šåŠ¨å‘ä¿¡æ¯\n')
                f.write('const newsData = ')
                f.write(json.dumps(existing_data, ensure_ascii=False, indent=4))
                f.write(';\n')
            
            print(f"âœ… æ•°æ®å·²æ›´æ–°åˆ° {data_file}")
            return True
        except Exception as e:
            print(f"âŒ å†™å…¥æ•°æ®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 50)
    print("æ•™åŸ¹è¡Œä¸šåŠ¨å‘è‡ªåŠ¨æ”¶é›†è„šæœ¬ - æµ‹è¯•æ¨¡å¼")
    print("=" * 50)
    print()
    
    collector = NewsCollector()
    
    # åˆ›å»ºæµ‹è¯•æ–°é—»
    print("ğŸ“° ç”Ÿæˆæµ‹è¯•æ–°é—»æ•°æ®...")
    news_list = collector.create_test_news()
    
    print(f"âœ… ç”Ÿæˆäº† {len(news_list)} æ¡æµ‹è¯•æ–°é—»")
    print()
    
    # æ˜¾ç¤ºæ–°é—»æ‘˜è¦
    print("ğŸ“‹ æ–°é—»æ‘˜è¦:")
    for i, news in enumerate(news_list, 1):
        print(f"  {i}. [{news['company']}] {news['title']}")
        print(f"     ä¼˜å…ˆçº§: {news['priority']} | åˆ†ç±»: {', '.join(news['categories'])}")
    print()
    
    # æ›´æ–°æ•°æ®æ–‡ä»¶
    print("ğŸ’¾ æ›´æ–°æ•°æ®æ–‡ä»¶...")
    if collector.update_data_file(news_list):
        print()
        print("=" * 50)
        print("âœ… æµ‹è¯•å®Œæˆï¼")
        print("=" * 50)
        print()
        print("ğŸ’¡ æç¤º:")
        print("  1. åˆ·æ–°æµè§ˆå™¨é¡µé¢æŸ¥çœ‹æ›´æ–°åçš„æ•°æ®")
        print("  2. å®é™…ä½¿ç”¨æ—¶ï¼Œéœ€è¦å®‰è£…ä¾èµ–: pip3 install -r requirements.txt")
        print("  3. ç„¶åè¿è¡Œå®Œæ•´ç‰ˆè„šæœ¬: python3 collect_news.py")
    else:
        print()
        print("âŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

if __name__ == '__main__':
    main()

