#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•™åŸ¹è¡Œä¸šåŠ¨å‘è‡ªåŠ¨æ”¶é›†è„šæœ¬ - çœŸå®é“¾æ¥ç‰ˆ
ä»å¤šä¸ªçœŸå®æ–°é—»æºè·å–æ•™è‚²è¡Œä¸šæ–°é—»ï¼Œç¡®ä¿æ¯æ¡æ–°é—»éƒ½æœ‰ç²¾ç¡®çš„åŸæ–‡é“¾æ¥
"""

import json
import re
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import time
import os
from urllib.parse import urljoin, quote

# é…ç½®ä¿¡æ¯
MAIN_COMPANIES = ['å¥½æœªæ¥', 'å¸Œæœ›å­¦', 'å­¦è€Œæ€', 'æ–°ä¸œæ–¹', 'ä½œä¸šå¸®', 'é«˜é€”', 'çŒ¿è¾…å¯¼']
YUANFUDAO_SUBSIDIARIES = ['æ–‘é©¬AI', 'æ–‘é©¬ç™¾ç§‘', 'çŒ¿ç¼–ç¨‹', 'é£è±¡æ˜Ÿçƒ', 'æ–‘é©¬', 'çŒ¿è¾…å¯¼']
EDUCATION_COMPANIES = MAIN_COMPANIES + YUANFUDAO_SUBSIDIARIES + [
    'VIPKID', 'æŒé—¨æ•™è‚²', 'ç½‘æ˜“æœ‰é“', 'è…¾è®¯æ•™è‚²', 'æœ‰é“', 'ç²‰ç¬”', 'ä¸­å…¬æ•™è‚²'
]

CATEGORIES = {
    'æ•™ç ”æˆæœ': ['æ•™ç ”', 'æ•™å­¦', 'è¯¾ç¨‹', 'æ•™å¸ˆ', 'æ•™å­¦æ–¹æ³•', 'è¯¾ç¨‹ä½“ç³»'],
    'AIæ•™è‚²': ['AI', 'äººå·¥æ™ºèƒ½', 'æ™ºèƒ½', 'å¤§æ¨¡å‹', 'GPT', 'ChatGPT', 'AI+æ•™è‚²'],
    'åŠ¨ç”»è§†é¢‘': ['åŠ¨ç”»', 'è§†é¢‘', '3D', 'åˆ¶ä½œ'],
    'è·å®¢å¢é•¿': ['è·å®¢', 'å¢é•¿', 'è¥é”€', 'æ¨å¹¿', 'æ‹›ç”Ÿ', 'å¸‚åœº'],
    'å“ç‰Œå…¬å…³': ['å“ç‰Œ', 'å…¬å…³', 'ä¼ æ’­', 'èèµ„', 'ä¸Šå¸‚', 'è´¢æŠ¥'],
    'ç”¨æˆ·è¿è¥': ['ç”¨æˆ·', 'è¿è¥', 'æ´»è·ƒ', 'ç•™å­˜'],
    'æ‹›è˜ä¿¡æ¯': ['æ‹›è˜', 'å²—ä½', 'èŒä½']
}


class RealNewsCollector:
    """çœŸå®æ–°é—»æ”¶é›†å™¨ - è·å–ç²¾ç¡®çš„æ–‡ç« é“¾æ¥"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
        self.collected_news = []
    
    def get_current_week(self):
        """è·å–å½“å‰å‘¨ä¿¡æ¯"""
        today = datetime.now()
        year, week, weekday = today.isocalendar()
        week_start = today - timedelta(days=weekday - 1)
        week_end = week_start + timedelta(days=6)
        week_str = f"{year}å¹´ç¬¬{week}å‘¨ ({week_start.strftime('%mæœˆ%dæ—¥')}-{week_end.strftime('%mæœˆ%dæ—¥')})"
        return week_str
    
    def extract_company(self, text):
        """ä»æ–‡æœ¬ä¸­æå–å…¬å¸åç§°"""
        for company in EDUCATION_COMPANIES:
            if company in text:
                # çŒ¿è¾…å¯¼ç³»åˆ—ç»Ÿä¸€å½’ç±»
                if company in YUANFUDAO_SUBSIDIARIES:
                    return company  # ä¿ç•™å…·ä½“å…¬å¸å
                return company
        return None
    
    def categorize_news(self, title, content=''):
        """æ ¹æ®æ ‡é¢˜å’Œå†…å®¹åˆ†ç±»"""
        categories = []
        full_text = (title + ' ' + content).lower()
        for category, keywords in CATEGORIES.items():
            for keyword in keywords:
                if keyword.lower() in full_text:
                    categories.append(category)
                    break
        return categories[:2] if categories else ['å…¶ä»–']
    
    def determine_priority(self, title, content=''):
        """åˆ¤æ–­ä¼˜å…ˆçº§"""
        high_keywords = ['å‘å¸ƒ', 'ä¸Šçº¿', 'èèµ„', 'ä¸Šå¸‚', 'è´¢æŠ¥', 'AI', 'äººå·¥æ™ºèƒ½', 
                        'çªç ´', 'åˆ›æ–°', 'é‡å¤§', 'æˆ˜ç•¥', 'åˆä½œ', 'æ”¶è´­']
        full_text = (title + ' ' + content).lower()
        for keyword in high_keywords:
            if keyword.lower() in full_text:
                return 'é‡ç‚¹'
        return 'æ¬¡é‡ç‚¹'
    
    # ==================== 36æ°ª ====================
    def collect_from_36kr(self, keyword):
        """ä»36æ°ªæœç´¢è·å–çœŸå®æ–‡ç« é“¾æ¥"""
        news_list = []
        try:
            # 36æ°ªæœç´¢API
            search_url = f"https://36kr.com/api/search-column/mainsite"
            params = {
                'per_page': 20,
                'page': 1,
                'keyword': keyword,
                'partner_id': 'web'
            }
            
            response = self.session.get(search_url, params=params, timeout=15)
            if response.status_code == 200:
                data = response.json()
                items = data.get('data', {}).get('items', [])
                
                for item in items:
                    try:
                        item_data = item.get('data', {}) if isinstance(item.get('data'), dict) else item
                        title = item_data.get('title', '') or item_data.get('catch_title', '')
                        article_id = item_data.get('id') or item_data.get('item_id')
                        
                        if not title or not article_id:
                            continue
                        
                        # æ„å»ºçœŸå®æ–‡ç« é“¾æ¥
                        real_url = f"https://36kr.com/p/{article_id}"
                        
                        # æå–å…¬å¸
                        company = self.extract_company(title)
                        if not company:
                            continue
                        
                        # è·å–æ‘˜è¦
                        summary = item_data.get('summary', '') or item_data.get('description', '')
                        if len(summary) > 200:
                            summary = summary[:200] + '...'
                        
                        # è·å–å‘å¸ƒæ—¶é—´
                        pub_time = item_data.get('published_at', '') or item_data.get('create_time', '')
                        if pub_time:
                            try:
                                date_str = datetime.fromisoformat(pub_time.replace('Z', '+00:00')).strftime('%Y-%m-%d')
                            except:
                                date_str = datetime.now().strftime('%Y-%m-%d')
                        else:
                            date_str = datetime.now().strftime('%Y-%m-%d')
                        
                        news_item = {
                            'type': 'æ–°é—»',
                            'company': company,
                            'title': title,
                            'content': summary,
                            'priority': self.determine_priority(title, summary),
                            'categories': self.categorize_news(title, summary),
                            'date': date_str,
                            'source': real_url  # çœŸå®æ–‡ç« é“¾æ¥
                        }
                        news_list.append(news_item)
                        
                    except Exception as e:
                        continue
                        
        except Exception as e:
            print(f"  36æ°ªæœç´¢å‡ºé”™: {e}")
        
        return news_list
    
    # ==================== èŠ¥æœ«å † ====================
    def collect_from_jiemodui(self):
        """ä»èŠ¥æœ«å †è·å–æ•™è‚²æ–°é—»"""
        news_list = []
        try:
            # èŠ¥æœ«å †é¦–é¡µå’Œåˆ—è¡¨é¡µ
            urls = [
                'https://www.jiemodui.com/',
                'https://www.jiemodui.com/N/0.html',  # æœ€æ–°èµ„è®¯
            ]
            
            for url in urls:
                try:
                    response = self.session.get(url, timeout=15)
                    response.encoding = 'utf-8'
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨
                    articles = soup.find_all('a', href=re.compile(r'/A/\d+\.html'))
                    
                    for article in articles[:30]:
                        try:
                            title = article.get_text(strip=True)
                            href = article.get('href', '')
                            
                            if not title or len(title) < 10:
                                continue
                            
                            # æ„å»ºå®Œæ•´URL
                            if href.startswith('/'):
                                real_url = f"https://www.jiemodui.com{href}"
                            elif href.startswith('http'):
                                real_url = href
                            else:
                                continue
                            
                            # æå–å…¬å¸
                            company = self.extract_company(title)
                            if not company:
                                continue
                            
                            news_item = {
                                'type': 'æ–°é—»',
                                'company': company,
                                'title': title,
                                'content': '',
                                'priority': self.determine_priority(title),
                                'categories': self.categorize_news(title),
                                'date': datetime.now().strftime('%Y-%m-%d'),
                                'source': real_url
                            }
                            news_list.append(news_item)
                            
                        except Exception:
                            continue
                    
                    time.sleep(1)
                except Exception:
                    continue
                    
        except Exception as e:
            print(f"  èŠ¥æœ«å †æŠ“å–å‡ºé”™: {e}")
        
        return news_list
    
    # ==================== å¤šçŸ¥ç½‘ ====================
    def collect_from_duozhi(self):
        """ä»å¤šçŸ¥ç½‘è·å–æ•™è‚²æ–°é—»"""
        news_list = []
        try:
            response = self.session.get('https://www.duozhi.com/', timeout=15)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–‡ç« é“¾æ¥
            articles = soup.find_all('a', href=re.compile(r'/news/\d+\.shtml'))
            
            for article in articles[:30]:
                try:
                    title = article.get_text(strip=True)
                    href = article.get('href', '')
                    
                    if not title or len(title) < 10:
                        continue
                    
                    # æ„å»ºå®Œæ•´URL
                    if href.startswith('/'):
                        real_url = f"https://www.duozhi.com{href}"
                    elif href.startswith('http'):
                        real_url = href
                    else:
                        continue
                    
                    # æå–å…¬å¸
                    company = self.extract_company(title)
                    if not company:
                        continue
                    
                    news_item = {
                        'type': 'æ–°é—»',
                        'company': company,
                        'title': title,
                        'content': '',
                        'priority': self.determine_priority(title),
                        'categories': self.categorize_news(title),
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'source': real_url
                    }
                    news_list.append(news_item)
                    
                except Exception:
                    continue
                    
        except Exception as e:
            print(f"  å¤šçŸ¥ç½‘æŠ“å–å‡ºé”™: {e}")
        
        return news_list
    
    # ==================== ç™¾åº¦æ–°é—»æœç´¢ ====================
    def collect_from_baidu_news(self, keyword):
        """ä»ç™¾åº¦æ–°é—»æœç´¢è·å–æ–°é—»"""
        news_list = []
        try:
            # ä½¿ç”¨ç™¾åº¦èµ„è®¯æœç´¢
            search_url = "https://www.baidu.com/s"
            params = {
                'wd': f'{keyword}',
                'tn': 'news',
                'ie': 'utf-8'
            }
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml',
                'Accept-Language': 'zh-CN,zh;q=0.9',
                'Referer': 'https://www.baidu.com/'
            }
            
            response = self.session.get(search_url, params=params, headers=headers, timeout=15)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»ç»“æœå®¹å™¨
            results = soup.find_all('div', class_=re.compile('result|c-container'))
            
            for result in results[:20]:
                try:
                    # è·å–æ ‡é¢˜å’Œé“¾æ¥
                    title_elem = result.find('a', href=True)
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    href = title_elem.get('href', '')
                    
                    if not title or not href or len(title) < 8:
                        continue
                    
                    # è§£æç™¾åº¦è·³è½¬é“¾æ¥è·å–çœŸå®URL
                    real_url = self._resolve_baidu_link(href)
                    if not real_url:
                        continue
                    
                    # æå–å…¬å¸
                    company = self.extract_company(title)
                    if not company:
                        continue
                    
                    # è·å–æ‘˜è¦
                    summary_elem = result.find(['div', 'span'], class_=re.compile('c-abstract|c-summary|content'))
                    summary = summary_elem.get_text(strip=True)[:200] if summary_elem else ''
                    
                    news_item = {
                        'type': 'æ–°é—»',
                        'company': company,
                        'title': title,
                        'content': summary,
                        'priority': self.determine_priority(title, summary),
                        'categories': self.categorize_news(title, summary),
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'source': real_url
                    }
                    news_list.append(news_item)
                    
                except Exception:
                    continue
                    
        except Exception as e:
            print(f"  ç™¾åº¦æ–°é—»æœç´¢å‡ºé”™: {e}")
        
        return news_list
    
    def _resolve_baidu_link(self, href):
        """è§£æç™¾åº¦è·³è½¬é“¾æ¥ï¼Œè·å–çœŸå®ç›®æ ‡URL"""
        try:
            if not href.startswith('http'):
                return None
            
            # è·Ÿéšé‡å®šå‘è·å–çœŸå®URL
            response = self.session.head(href, allow_redirects=True, timeout=8)
            final_url = response.url
            
            # éªŒè¯URLæœ‰æ•ˆæ€§
            if final_url and final_url.startswith('http') and 'baidu.com' not in final_url:
                return final_url
            
            # å¦‚æœheadè¯·æ±‚å¤±è´¥ï¼Œå°è¯•getè¯·æ±‚
            response = self.session.get(href, allow_redirects=True, timeout=8)
            return response.url if response.url and 'baidu.com' not in response.url else None
            
        except Exception:
            return None
    
    # ==================== å¿…åº”æ–°é—»æœç´¢ ====================
    def collect_from_bing_news(self, keyword):
        """ä»å¿…åº”æ–°é—»æœç´¢è·å–æ–°é—»"""
        news_list = []
        try:
            search_url = "https://cn.bing.com/news/search"
            params = {
                'q': f'{keyword} æ•™è‚²',
                'FORM': 'HDRSC6'
            }
            
            response = self.session.get(search_url, params=params, timeout=15)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»å¡ç‰‡
            cards = soup.find_all('a', class_='title')
            
            for card in cards[:20]:
                try:
                    title = card.get_text(strip=True)
                    href = card.get('href', '')
                    
                    if not title or not href:
                        continue
                    
                    # æå–å…¬å¸
                    company = self.extract_company(title)
                    if not company:
                        continue
                    
                    news_item = {
                        'type': 'æ–°é—»',
                        'company': company,
                        'title': title,
                        'content': '',
                        'priority': self.determine_priority(title),
                        'categories': self.categorize_news(title),
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'source': href
                    }
                    news_list.append(news_item)
                    
                except Exception:
                    continue
                    
        except Exception as e:
            print(f"  å¿…åº”æ–°é—»æœç´¢å‡ºé”™: {e}")
        
        return news_list
    
    # ==================== æœç‹—æ–°é—»æœç´¢ ====================
    def collect_from_sogou_news(self, keyword):
        """ä»æœç‹—æ–°é—»æœç´¢è·å–æ–°é—» - è§£æçœŸå®é“¾æ¥"""
        news_list = []
        try:
            search_url = "https://news.sogou.com/news"
            params = {
                'query': f'{keyword}',
                'mode': 1,
                'sort': 0
            }
            
            response = self.session.get(search_url, params=params, timeout=15)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»ç»“æœ
            results = soup.find_all('div', class_='vrwrap')
            if not results:
                results = soup.find_all('div', class_='news-item')
            if not results:
                results = soup.find_all('li', class_='news150507')
            
            for result in results[:15]:
                try:
                    # è·å–æ ‡é¢˜å’Œé“¾æ¥
                    title_elem = result.find('a')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    href = title_elem.get('href', '')
                    
                    if not title or not href or len(title) < 8:
                        continue
                    
                    # ä¿®å¤é“¾æ¥æ ¼å¼å¹¶è·å–çœŸå®é“¾æ¥
                    real_url = self._resolve_sogou_link(href)
                    if not real_url:
                        continue
                    
                    # æå–å…¬å¸
                    company = self.extract_company(title)
                    if not company:
                        continue
                    
                    # è·å–æ‘˜è¦
                    summary_elem = result.find(['p', 'div'], class_=re.compile('content|summary|txt'))
                    summary = summary_elem.get_text(strip=True)[:200] if summary_elem else ''
                    
                    news_item = {
                        'type': 'æ–°é—»',
                        'company': company,
                        'title': title,
                        'content': summary,
                        'priority': self.determine_priority(title, summary),
                        'categories': self.categorize_news(title, summary),
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'source': real_url  # ä½¿ç”¨è§£æåçš„çœŸå®é“¾æ¥
                    }
                    news_list.append(news_item)
                    
                except Exception:
                    continue
                    
        except Exception as e:
            print(f"  æœç‹—æ–°é—»æœç´¢å‡ºé”™: {e}")
        
        return news_list
    
    def _resolve_sogou_link(self, href):
        """è§£ææœç‹—è·³è½¬é“¾æ¥ï¼Œè·å–çœŸå®ç›®æ ‡URL"""
        try:
            # æ„å»ºå®Œæ•´URL
            if href.startswith('/link'):
                full_url = f"https://news.sogou.com{href}"
            elif href.startswith('http'):
                full_url = href
            else:
                return None
            
            # è¯·æ±‚é“¾æ¥å¹¶è·Ÿéšé‡å®šå‘
            response = self.session.get(full_url, allow_redirects=True, timeout=10)
            final_url = response.url
            
            # éªŒè¯æœ€ç»ˆURLæœ‰æ•ˆ
            if final_url and final_url.startswith('http') and 'sogou.com' not in final_url:
                return final_url
            
            # å¦‚æœé‡å®šå‘å¤±è´¥ï¼Œå°è¯•ä»é¡µé¢å†…å®¹æå–çœŸå®é“¾æ¥
            if 'sogou.com' in final_url:
                soup = BeautifulSoup(response.text, 'html.parser')
                # æŸ¥æ‰¾å¯èƒ½çš„é‡å®šå‘é“¾æ¥
                meta_refresh = soup.find('meta', attrs={'http-equiv': 'refresh'})
                if meta_refresh:
                    content = meta_refresh.get('content', '')
                    url_match = re.search(r'url=([^\s"\']+)', content, re.IGNORECASE)
                    if url_match:
                        return url_match.group(1)
            
            return None
        except Exception as e:
            return None
    
    # ==================== ä»Šæ—¥å¤´æ¡æœç´¢ ====================
    def collect_from_toutiao(self, keyword):
        """ä»ä»Šæ—¥å¤´æ¡æœç´¢è·å–æ–°é—»"""
        news_list = []
        try:
            search_url = "https://so.toutiao.com/search"
            params = {
                'keyword': keyword,
                'pd': 'information',
                'source': 'input'
            }
            
            response = self.session.get(search_url, params=params, timeout=15)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–‡ç« é“¾æ¥
            articles = soup.find_all('a', href=re.compile(r'toutiao\.com|toutiaocdn\.com'))
            
            for article in articles[:15]:
                try:
                    title = article.get_text(strip=True)
                    href = article.get('href', '')
                    
                    if not title or not href or len(title) < 8:
                        continue
                    
                    # æå–å…¬å¸
                    company = self.extract_company(title)
                    if not company:
                        continue
                    
                    news_item = {
                        'type': 'æ–°é—»',
                        'company': company,
                        'title': title,
                        'content': '',
                        'priority': self.determine_priority(title),
                        'categories': self.categorize_news(title),
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'source': href
                    }
                    news_list.append(news_item)
                    
                except Exception:
                    continue
                    
        except Exception as e:
            print(f"  ä»Šæ—¥å¤´æ¡æœç´¢å‡ºé”™: {e}")
        
        return news_list
    
    def verify_url(self, url):
        """éªŒè¯URLæ˜¯å¦å¯è®¿é—®"""
        try:
            response = self.session.head(url, timeout=5, allow_redirects=True)
            return response.status_code < 400
        except:
            return False
    
    def collect_all_news(self):
        """æ”¶é›†æ‰€æœ‰æ–°é—»"""
        print("=" * 60)
        print("æ•™åŸ¹è¡Œä¸šåŠ¨å‘è‡ªåŠ¨æ”¶é›† - çœŸå®é“¾æ¥ç‰ˆ")
        print("=" * 60)
        print()
        
        all_news = []
        
        # 1. ä»36æ°ªæœç´¢
        print("ğŸ“° ä»36æ°ªæœç´¢æ–°é—»...")
        for company in MAIN_COMPANIES:
            print(f"  æœç´¢: {company}")
            news = self.collect_from_36kr(company)
            all_news.extend(news)
            print(f"    è·å– {len(news)} æ¡")
            time.sleep(1)
        
        # 2. ä»èŠ¥æœ«å †è·å–
        print("\nğŸ“° ä»èŠ¥æœ«å †è·å–æ–°é—»...")
        news = self.collect_from_jiemodui()
        all_news.extend(news)
        print(f"  è·å– {len(news)} æ¡")
        
        # 3. ä»å¤šçŸ¥ç½‘è·å–
        print("\nğŸ“° ä»å¤šçŸ¥ç½‘è·å–æ–°é—»...")
        news = self.collect_from_duozhi()
        all_news.extend(news)
        print(f"  è·å– {len(news)} æ¡")
        
        # 4. ä»ç™¾åº¦æ–°é—»æœç´¢ï¼ˆä¸»è¦æ¥æºï¼‰
        print("\nğŸ“° ä»ç™¾åº¦æ–°é—»æœç´¢...")
        for company in MAIN_COMPANIES:
            print(f"  æœç´¢: {company}")
            news = self.collect_from_baidu_news(company)
            all_news.extend(news)
            print(f"    è·å– {len(news)} æ¡")
            time.sleep(2)  # ç¨é•¿å»¶è¿Ÿé¿å…è¢«å°
        
        # 5. ä»æœç‹—æ–°é—»æœç´¢è¡¥å……
        print("\nğŸ“° ä»æœç‹—æ–°é—»æœç´¢è¡¥å……...")
        for company in MAIN_COMPANIES[:3]:  # åªæœå‰3ä¸ª
            print(f"  æœç´¢: {company}")
            news = self.collect_from_sogou_news(company)
            all_news.extend(news)
            print(f"    è·å– {len(news)} æ¡")
            time.sleep(1)
        
        # å»é‡
        print("\nğŸ”„ å»é‡å¤„ç†...")
        seen_titles = set()
        unique_news = []
        for news in all_news:
            title_key = news['title'][:30]
            if title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_news.append(news)
        
        print(f"\nâœ… å…±æ”¶é›† {len(unique_news)} æ¡ç‹¬ç«‹æ–°é—»ï¼ˆåŸå§‹ {len(all_news)} æ¡ï¼‰")
        
        return unique_news
    
    def update_data_file(self, news_list):
        """æ›´æ–°data.jsæ–‡ä»¶"""
        week_str = self.get_current_week()
        data_file = os.path.join(os.path.dirname(__file__), 'data.js')
        
        # è¯»å–ç°æœ‰æ•°æ®
        existing_data = []
        if os.path.exists(data_file):
            try:
                with open(data_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    json_match = re.search(r'const newsData = (\[.*?\]);', content, re.DOTALL)
                    if json_match:
                        existing_data = json.loads(json_match.group(1))
            except Exception as e:
                print(f"è¯»å–ç°æœ‰æ•°æ®æ—¶å‡ºé”™: {e}")
        
        # æŸ¥æ‰¾æˆ–åˆ›å»ºæœ¬å‘¨æ•°æ®
        current_week_data = None
        for week_data in existing_data:
            if week_data['week'] == week_str:
                current_week_data = week_data
                break
        
        if current_week_data:
            # åˆå¹¶æ–°é—»
            existing_titles = {n.get('title', '')[:30] for n in current_week_data['news']}
            new_news = [n for n in news_list if n.get('title', '')[:30] not in existing_titles]
            current_week_data['news'].extend(new_news)
            print(f"æœ¬å‘¨å·²æœ‰æ•°æ®ï¼Œæ–°å¢ {len(new_news)} æ¡æ–°é—»")
        else:
            new_week_data = {
                'week': week_str,
                'news': news_list
            }
            existing_data.append(new_week_data)
            print(f"åˆ›å»ºæ–°çš„å‘¨æ•°æ®: {week_str}")
        
        # æŒ‰å‘¨æ’åº
        existing_data.sort(key=lambda x: x['week'])
        
        # åªä¿ç•™æœ€è¿‘13å‘¨
        existing_data = existing_data[-13:]
        
        # å†™å…¥æ–‡ä»¶
        try:
            with open(data_file, 'w', encoding='utf-8') as f:
                f.write('// data.js - æ–°é—»æ•°æ®\n')
                f.write('// æ•™åŸ¹è¡Œä¸šåŠ¨å‘ä¿¡æ¯ - çœŸå®é“¾æ¥ç‰ˆ\n')
                f.write('// æ‰€æœ‰é“¾æ¥å‡ä¸ºçœŸå®æ–‡ç« é“¾æ¥ï¼Œå¯ç›´æ¥è®¿é—®åŸæ–‡\n')
                f.write('console.log("ğŸ“¦ data.js å¼€å§‹æ‰§è¡Œ...");\n')
                f.write('const newsData = ')
                f.write(json.dumps(existing_data, ensure_ascii=False, indent=4))
                f.write(';\n')
                f.write('console.log("âœ… data.js æ‰§è¡Œå®Œæˆ, newsDataé•¿åº¦:", newsData.length);\n')
                f.write('window.newsData = newsData;\n')
            
            print(f"âœ… æ•°æ®å·²æ›´æ–°åˆ° {data_file}")
            return True
        except Exception as e:
            print(f"âŒ å†™å…¥æ•°æ®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    collector = RealNewsCollector()
    
    # æ”¶é›†æ–°é—»
    news_list = collector.collect_all_news()
    
    if news_list:
        # æ›´æ–°æ•°æ®æ–‡ä»¶
        collector.update_data_file(news_list)
        
        # ç»Ÿè®¡
        companies = {}
        for news in news_list:
            company = news.get('company', 'æœªçŸ¥')
            companies[company] = companies.get(company, 0) + 1
        
        print("\nğŸ“Š å„å…¬å¸æ–°é—»æ•°é‡:")
        for company, count in sorted(companies.items(), key=lambda x: -x[1]):
            print(f"  {company}: {count} æ¡")
    else:
        print("\nâš ï¸ æœªæ”¶é›†åˆ°ä»»ä½•æ–°é—»")
    
    print("\n" + "=" * 60)
    print("æ•°æ®æ”¶é›†å®Œæˆ")
    print("=" * 60)


if __name__ == '__main__':
    main()

