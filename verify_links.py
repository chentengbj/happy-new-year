#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éªŒè¯data.jsä¸­æ‰€æœ‰é“¾æŽ¥çš„æœ‰æ•ˆæ€§
"""

import re
import json
import requests
from urllib.parse import urlparse
import time

def verify_link(url):
    """éªŒè¯é“¾æŽ¥æ˜¯å¦å¯è®¿é—®"""
    if not url or not url.startswith('http'):
        return False, 'æ— æ•ˆURLæ ¼å¼'
    
    try:
        parsed = urlparse(url)
        if 'example.com' in parsed.netloc:
            return False, 'ç¤ºä¾‹é“¾æŽ¥'
        
        # å°è¯•è®¿é—®é“¾æŽ¥
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=5, allow_redirects=True)
        
        if response.status_code == 200:
            return True, 'å¯è®¿é—®'
        else:
            return False, f'HTTP {response.status_code}'
    except requests.exceptions.Timeout:
        return False, 'è¯·æ±‚è¶…æ—¶'
    except requests.exceptions.ConnectionError:
        return False, 'è¿žæŽ¥å¤±è´¥'
    except Exception as e:
        return False, f'é”™è¯¯: {str(e)}'

def main():
    # è¯»å–data.jsæ–‡ä»¶
    with open('data.js', 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æå–JSONæ•°æ®
    json_match = re.search(r'const newsData = (\[.*?\]);', content, re.DOTALL)
    if not json_match:
        print("âŒ æ— æ³•æ‰¾åˆ°newsDataæ•°æ®")
        return
    
    json_str = json_match.group(1)
    data = json.loads(json_str)
    
    print("=" * 60)
    print("å¼€å§‹éªŒè¯æ‰€æœ‰é“¾æŽ¥...")
    print("=" * 60)
    print()
    
    invalid_count = 0
    valid_count = 0
    invalid_links = []
    
    # éªŒè¯æ‰€æœ‰é“¾æŽ¥
    for week_idx, week_data in enumerate(data):
        week = week_data.get('week', '')
        news_list = week_data.get('news', [])
        
        print(f"ðŸ“… {week} - å…± {len(news_list)} æ¡ä¿¡æ¯")
        
        for news_idx, news in enumerate(news_list):
            source = news.get('source', '')
            title = news.get('title', news.get('jobTitle', 'æœªçŸ¥'))
            company = news.get('company', 'æœªçŸ¥')
            
            if not source:
                print(f"  âŒ [{news_idx+1}] {company} - {title[:30]}... - ç¼ºå°‘æ¥æºé“¾æŽ¥")
                invalid_count += 1
                invalid_links.append({
                    'week': week,
                    'company': company,
                    'title': title,
                    'source': source,
                    'reason': 'ç¼ºå°‘æ¥æºé“¾æŽ¥'
                })
                continue
            
            is_valid, reason = verify_link(source)
            
            if is_valid:
                print(f"  âœ… [{news_idx+1}] {company} - {title[:30]}...")
                valid_count += 1
            else:
                print(f"  âŒ [{news_idx+1}] {company} - {title[:30]}... - {reason}")
                print(f"      é“¾æŽ¥: {source}")
                invalid_count += 1
                invalid_links.append({
                    'week': week,
                    'company': company,
                    'title': title,
                    'source': source,
                    'reason': reason
                })
            
            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        print()
    
    print("=" * 60)
    print(f"éªŒè¯å®Œæˆï¼")
    print(f"âœ… æœ‰æ•ˆé“¾æŽ¥: {valid_count}")
    print(f"âŒ æ— æ•ˆé“¾æŽ¥: {invalid_count}")
    print("=" * 60)
    
    if invalid_links:
        print("\næ— æ•ˆé“¾æŽ¥è¯¦æƒ…ï¼š")
        for item in invalid_links[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  - {item['company']}: {item['title'][:40]}...")
            print(f"    åŽŸå› : {item['reason']}")
            print(f"    é“¾æŽ¥: {item['source']}")
            print()
        
        if len(invalid_links) > 10:
            print(f"  ... è¿˜æœ‰ {len(invalid_links) - 10} ä¸ªæ— æ•ˆé“¾æŽ¥")

if __name__ == '__main__':
    main()

